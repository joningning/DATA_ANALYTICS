{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52b0b8-a383-41a6-bead-496e481d91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "\n",
    "soybean = fetch_ucirepo(id=91) \n",
    "zoo = fetch_ucirepo(id=111)\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "dermatology = fetch_ucirepo(id=33)\n",
    "breast_cancer = fetch_ucirepo(id=15)\n",
    "mushroom = fetch_ucirepo(id=73)\n",
    "\n",
    "X = soybean.data.features\n",
    "y = soybean.data.targets \n",
    "soybean_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "\n",
    "X = zoo.data.features\n",
    "y = zoo.data.targets \n",
    "zoo_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "\n",
    "X = heart_disease.data.features\n",
    "y = heart_disease.data.targets \n",
    "heart_disease_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "\n",
    "X = dermatology.data.features\n",
    "y = dermatology.data.targets \n",
    "dermatology_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "\n",
    "X = breast_cancer.data.features\n",
    "y = breast_cancer.data.targets \n",
    "breast_cancer_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "\n",
    "X = mushroom.data.features\n",
    "y = mushroom.data.targets \n",
    "mushroom_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "soybean_df = soybean_df.dropna()\n",
    "zoo_df = zoo_df.dropna()\n",
    "heart_disease_df = heart_disease_df.dropna()\n",
    "dermatology_df = dermatology_df.dropna()\n",
    "breast_cancer_df = breast_cancer_df.dropna()\n",
    "mushroom_df = mushroom_df.dropna()\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class CustomBernoulliSEM:\n",
    "    \n",
    "    def __init__(self, num_components, max_iterations, batch_size=100, tolerance=1e-3):\n",
    "        self.num_components = num_components\n",
    "        self.max_iterations = max_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.tolerance = tolerance\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        self.initialize_parameters()\n",
    "        log_bernoullis = self.calculate_log_bernoullis(self.data)\n",
    "        self.previous_log_likelihood = self.calculate_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iterations):\n",
    "            if step > 0:\n",
    "                self.previous_log_likelihood = self.log_likelihood\n",
    "            self.responsibilities = self.calculate_responsibilities(log_bernoullis)\n",
    "            self.save_previous_parameters()\n",
    "            self.update_Neff()\n",
    "            self.update_mu(self.data)\n",
    "            self.update_pi()\n",
    "            log_bernoullis = self.calculate_log_bernoullis(self.data)\n",
    "            self.log_likelihood = self.calculate_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.log_likelihood):\n",
    "                self.reset_parameters()\n",
    "                print(self.log_likelihood)\n",
    "                break\n",
    "\n",
    "    def iterate_batches(self):\n",
    "        num_samples = len(self.data)\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, num_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, num_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            yield self.data.iloc[batch_indices]\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.mu = self.previous_mu.copy()\n",
    "        self.pi = self.previous_pi.copy()\n",
    "        self.responsibilities = self.previous_responsibilities.copy()\n",
    "        self.update_Neff()\n",
    "        log_bernoullis = self.calculate_log_bernoullis(self.data)\n",
    "        self.log_likelihood = self.calculate_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def save_previous_parameters(self):\n",
    "        self.previous_mu = self.mu.copy()\n",
    "        self.previous_pi = self.pi.copy()\n",
    "        self.previous_responsibilities = self.responsibilities.copy()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        self.num_samples = self.data.shape[0]\n",
    "        self.num_features = self.data.shape[1]\n",
    "        self.pi = 1/self.num_components * np.ones(self.num_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.num_components, self.num_features))\n",
    "        self.normalize_mu()\n",
    "        self.previous_mu = None\n",
    "        self.previous_pi = None\n",
    "        self.previous_responsibilities = None\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.num_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def calculate_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.num_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.num_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def calculate_log_bernoullis(self, data):\n",
    "        log_bernoullis = self.calculate_log_prob(data, self.mu)\n",
    "        log_bernoullis += self.calculate_log_prob(1-data, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def calculate_log_prob(self, data, mu):\n",
    "        epsilon = 1e-15\n",
    "        mu_place = np.clip(mu, epsilon, 1 - epsilon)\n",
    "        return np.tensordot(data, np.log(mu_place), (1,1))\n",
    "\n",
    "    def update_Neff(self):\n",
    "        self.Neff = np.sum(self.responsibilities, axis=0)\n",
    "    \n",
    "    def update_mu(self, batch):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.responsibilities, batch) / self.Neff[:, None]\n",
    "        \n",
    "    def update_pi(self):\n",
    "        self.pi = self.Neff / self.num_samples\n",
    "    \n",
    "    def predict(self, data):\n",
    "        log_bernoullis = self.calculate_log_bernoullis(data)\n",
    "        gamma = self.calculate_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def calculate_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, data):\n",
    "        log_bernoullis = self.calculate_log_bernoullis(data)\n",
    "        return self.calculate_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, data):\n",
    "        log_bernoullis = self.calculate_log_bernoullis(data)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd76b4f-9636-4981-af92-d2a285d0c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class CustomKMeans(KMeans):\n",
    "    pass\n",
    "\n",
    "def custom_encode_categorical(dataframe):\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = dataframe.copy()\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].dtype == 'object':\n",
    "            encoded_df[col] = encoder.fit_transform(dataframe[col])\n",
    "    return encoded_df\n",
    "\n",
    "datasets_list = [\"soybean_df\", \"zoo_df\", \"heart_disease_df\", \"dermatology_df\", \"breast_cancer_df\", \"mushroom_df\"]\n",
    "encoded_data = {}\n",
    "\n",
    "for dataset in datasets_list:\n",
    "    df_copy = globals()[dataset].copy()\n",
    "    encoded_data[dataset] = custom_encode_categorical(df_copy)\n",
    "\n",
    "def evaluate_clustering_algo(dataset_name, algorithm_name, **kwargs):\n",
    "    data = encoded_data[dataset_name]\n",
    "    features = data.iloc[:, :-1]\n",
    "    true_labels = data.iloc[:, -1]\n",
    "    \n",
    "    if algorithm_name == 'CustomKMeans':\n",
    "        model = CustomKMeans(**kwargs)\n",
    "        labels = model.fit_predict(features)\n",
    "    elif algorithm_name == 'CustomBernoulliSEM':\n",
    "        model = CustomBernoulliSEM(**kwargs)\n",
    "        model.fit(features)\n",
    "        labels = model.predict(features)\n",
    "    else:\n",
    "        raise ValueError(\"Algorithm not supported.\")\n",
    "    \n",
    "    fmi, ari, nmi = None, None, None\n",
    "    \n",
    "    if true_labels is not None:\n",
    "        fmi = fowlkes_mallows_score(true_labels, labels)\n",
    "        ari = adjusted_rand_score(true_labels, labels)\n",
    "        nmi = normalized_mutual_info_score(true_labels, labels)\n",
    "    \n",
    "    return fmi, ari, nmi\n",
    "\n",
    "results_dict = {}\n",
    "algorithms_list = ['CustomKMeans', 'CustomBernoulliSEM']\n",
    "for dataset_name in datasets_list:\n",
    "    results_dict[dataset_name] = {}\n",
    "    for algorithm in algorithms_list:\n",
    "        if algorithm == 'CustomKMeans':\n",
    "            kwargs = {'n_clusters': 2, 'max_iter': 100, 'n_init': 10}  # Explicitly set n_init\n",
    "        elif algorithm == 'CustomBernoulliSEM':\n",
    "            kwargs = {'num_components': 2, 'max_iterations': 100}\n",
    "        fmi_score, ari_score, nmi_score = evaluate_clustering_algo(dataset_name, algorithm, **kwargs)\n",
    "        results_dict[dataset_name][algorithm] = {'FMI': fmi_score, 'ARI': ari_score, 'NMI': nmi_score}\n",
    "\n",
    "print(results_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841d2d83-cf61-435c-80cb-adb9261a5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for results\n",
    "results_data = []\n",
    "\n",
    "for dataset_name, algorithms in results_dict.items():\n",
    "    for algorithm, metrics in algorithms.items():\n",
    "        results_data.append([dataset_name, algorithm, metrics['FMI'], metrics['ARI'], metrics['NMI']])\n",
    "\n",
    "# Create DataFrame from results data\n",
    "results_dataframe = pd.DataFrame(results_data, columns=['Dataset', 'Algorithm', 'FMI', 'ARI', 'NMI'])\n",
    "results_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe060d4a-0611-4ffe-9e14-092196e5be94",
   "metadata": {},
   "source": [
    "The FMI is often greater than the ARI and NMI because it focuses directly on the balance of precision and recall without adjusting for chance or information overlap, making it simpler and often yielding higher values. The problem with ARI is that it adjusts for random chance, which can lower its value, especially in complex or unbalanced clusterings. NMI, on the other hand, measures the shared information between clusterings, but this can be biased towards larger clusters and harder to interpret, often resulting in lower values compared to FMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a317a65-1167-4c43-9fb8-64f5fceabd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

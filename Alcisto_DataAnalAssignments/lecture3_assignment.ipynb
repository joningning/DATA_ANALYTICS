{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfb59d-4a79-404f-9093-afd1055ceb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "soybean = fetch_ucirepo(id=91) \n",
    "zoo = fetch_ucirepo(id=111)\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "dermatology = fetch_ucirepo(id=33)\n",
    "breast_cancer = fetch_ucirepo(id=15)\n",
    "mushroom = fetch_ucirepo(id=73)\n",
    "\n",
    "soybean_data = pd.DataFrame(data=soybean.data.features, columns=soybean.data.feature_names)\n",
    "soybean_data['target'] = soybean.data.targets\n",
    "\n",
    "zoo_data = pd.DataFrame(data=zoo.data.features, columns=zoo.data.feature_names)\n",
    "zoo_data['target'] = zoo.data.targets\n",
    "\n",
    "kokoro_data = pd.DataFrame(data=heart_disease.data.features, columns=heart_disease.data.feature_names)\n",
    "kokoro_data['target'] = heart_disease.data.targets\n",
    "\n",
    "dermatology_data = pd.DataFrame(data=dermatology.data.features, columns=dermatology.data.feature_names)\n",
    "dermatology_df['target'] = dermatology.data.targets\n",
    "\n",
    "breasts_data = pd.DataFrame(data=breast_cancer.data.features, columns=breast_cancer.data.feature_names)\n",
    "breasts_data['target'] = breast_cancer.data.targets\n",
    "\n",
    "mushroom_data = pd.DataFrame(data=mushroom.data.features, columns=mushroom.data.feature_names)\n",
    "mushroom_data['target'] = mushroom.data.targets\n",
    "\n",
    "def jaccard_coefficient(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "def ochiai_coefficient(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    denominator = np.sqrt(len(set1) * len(set2))\n",
    "    return intersection / denominator\n",
    "\n",
    "def overlap_coefficient(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    min_length = min(len(set1), len(set2))\n",
    "    return intersection / min_length\n",
    "\n",
    "def dice_coefficient(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    denominator = len(set1) + len(set2)\n",
    "    return 2 * intersection / denominator\n",
    "\n",
    "def graph_based_representation(data):\n",
    "    num_samples, num_features = data.shape\n",
    "    similarity_matrix = np.zeros((num_features, num_features))\n",
    "    for i, j in itertools.combinations(range(num_features), 2):\n",
    "        similarity_matrix[i, j] = jaccard_coefficient(set(data[:, i]), set(data[:, j]))\n",
    "        similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "    G = nx.from_numpy_array(similarity_matrix)\n",
    "    embedding = SpectralEmbedding(n_components=p)\n",
    "    representation_matrix = embedding.fit_transform(similarity_matrix)\n",
    "    return representation_matrix\n",
    "\n",
    "def joint_operation(data, representation_matrix):\n",
    "    return np.dot(data, representation_matrix)\n",
    "\n",
    "def mean_operation(data, representation_matrix):\n",
    "    return np.mean(np.dot(data, representation_matrix), axis=1)\n",
    "\n",
    "def perform_clustering(data, k):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    return kmeans.fit_predict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dac6f-3dfc-4457-b94a-e6b4fc2662a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
    "\n",
    "def jaccard_coefficient(set1, set2):\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def create_graph_representation(data):\n",
    "    num_samples, num_features = data.shape\n",
    "    similarity_matrix = np.zeros((num_features, num_features))\n",
    "    for i, j in itertools.combinations(range(num_features), 2):\n",
    "        similarity_matrix[i, j] = jaccard_coefficient(set(data[:, i]), set(data[:, j]))\n",
    "        similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "    G = nx.from_numpy_array(similarity_matrix)\n",
    "    embedding = SpectralEmbedding(n_components=num_components)\n",
    "    representation_matrix = embedding.fit_transform(similarity_matrix)\n",
    "    return representation_matrix\n",
    "\n",
    "def apply_joint_operation(data, representation_matrix):\n",
    "    return np.dot(data, representation_matrix)\n",
    "\n",
    "def cluster_data(data, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=10)  # Explicitly set n_init\n",
    "    return kmeans.fit_predict(data)\n",
    "\n",
    "num_components = 10\n",
    "num_clusters = 3\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "datasets = {\n",
    "     \"soybean_data\": soybean_data,\n",
    "     \"zoo_data\": zoo_data,\n",
    "     \"kokoro_data\": kokoro_data,\n",
    "     \"dermatology_df\": dermatology_df,\n",
    "     \"breasts_data\": breast_data,\n",
    "     \"mushroom_df\": mushroom_df\n",
    " }\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "        try:\n",
    "            X = dataset.drop(columns=[dataset.columns[-1]])\n",
    "            y = dataset[dataset.columns[-1]] \n",
    "            encoder = OneHotEncoder()\n",
    "            X_encoded = encoder.fit_transform(X)\n",
    "            representation_matrix = create_graph_representation(X_encoded.toarray())\n",
    "            integrated_data = apply_joint_operation(X_encoded.toarray(), representation_matrix)\n",
    "            labels = cluster_data(integrated_data, num_clusters)\n",
    "\n",
    "            ARI = adjusted_rand_score(y, labels)\n",
    "            NMI = normalized_mutual_info_score(y, labels)\n",
    "            FMI = fowlkes_mallows_score(y, labels)\n",
    "\n",
    "            performance_results.append([dataset_name, ARI, NMI, FMI])\n",
    "        except UserWarning as e:\n",
    "            print(f\"Warning: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(performance_results, columns=[\"Dataset\", \"ARI\", \"NMI\", \"FMI\"])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a5de3-0dff-40ef-9069-c71ac65e1b8e",
   "metadata": {},
   "source": [
    "The ARI, NMI, and FMI are ways to measure how good your clustering results are. ARI adjusts for random chance and works well when your clusters are similar in size, but it can be complicated. NMI checks how much information two clusterings share, so it's good for clusters of different sizes but can be a bit hard to understand. FMI is simpler and faster, balancing how well clusters group together (precision) and how completely they do so (recall), but it's not as good with clusters of very different sizes. Use ARI for balanced clusters, NMI for varied sizes, and FMI for quick, simple checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d03bf-914c-4695-941e-0c59f3109820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "k = 3\n",
    "def preprocess_and_encode_categorical_data(data):\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_data = data.apply(encoder.fit_transform)\n",
    "    return encoded_data\n",
    "\n",
    "def perform_clustering_and_evaluate_performance(data, true_labels, num_clusters):\n",
    "    km = KModes(n_clusters=num_clusters, init='Huang', n_init=5, verbose=0)\n",
    "    km_labels = km.fit_predict(data)\n",
    "    ARI_km = adjusted_rand_score(true_labels, km_labels)\n",
    "    NMI_km = normalized_mutual_info_score(true_labels, km_labels)\n",
    "    FMI_km = fowlkes_mallows_score(true_labels, km_labels)\n",
    "\n",
    "    ac = AgglomerativeClustering(n_clusters=num_clusters, linkage='ward')\n",
    "    ac_labels = ac.fit_predict(data)\n",
    "    ARI_ac = adjusted_rand_score(true_labels, ac_labels)\n",
    "    NMI_ac = normalized_mutual_info_score(true_labels, ac_labels)\n",
    "    FMI_ac = fowlkes_mallows_score(true_labels, ac_labels)\n",
    "    \n",
    "    return [\n",
    "        [\"K-Modes\", ARI_km, NMI_km, FMI_km],\n",
    "        [\"Hierarchical\", ARI_ac, NMI_ac, FMI_ac]\n",
    "    ]\n",
    "\n",
    "results_categorical = []\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    dataset = globals()[dataset_name]\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        try:\n",
    "            X_cat = dataset.iloc[:, :-1]\n",
    "            true_labels_cat = dataset.iloc[:, -1]\n",
    "\n",
    "            X_cat_encoded = preprocess_and_encode_categorical_data(X_cat)\n",
    "            clustering_results = perform_clustering_and_evaluate_performance(X_cat_encoded, true_labels_cat, k)  # Pass 'k' as parameter\n",
    "\n",
    "            for method, ari, nmi, fmi in clustering_results:\n",
    "                results_categorical.append([dataset_name + \" (\" + method + \")\", ari, nmi, fmi])\n",
    "        except UserWarning as e:\n",
    "            print(f\"Warning: {e}\")\n",
    "\n",
    "results_categorical_df = pd.DataFrame(results_categorical, columns=[\"Dataset\", \"ARI\", \"NMI\", \"FMI\"])\n",
    "print(results_categorical_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15b9d0-2b04-4a43-949f-4961c4535af7",
   "metadata": {},
   "source": [
    "The FMI is often greater than the Adjusted Rand Index ARI and Normalized Mutual Information NMI because it focuses directly on the balance of precision and recall without adjusting for chance or information overlap, making it simpler and often yielding higher values. The problem with ARI is that it adjusts for random chance, which can lower its value, especially in complex or unbalanced clusterings. NMI, on the other hand, measures the shared information between clusterings, but this can be biased towards larger clusters and harder to interpret, often resulting in lower values compared to FMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255376e0-c1e2-4e6d-8104-5beb90314324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
